# Aligning Large Language Models with Preference Privacy

This repository contains code implementing the methods described in the paper:

> **[Aligning Large Language Models with Preference Privacy]**

---

## ğŸ“˜ Overview

This project explores **preference-level privacy** in aligning large language models (LLMs) using human feedback. It implements:

- **Randomized Response (RR)**: Adds label-level noise to ensure differential privacy for human preferences.
- **DP-Stochastic Gradient Descent (DP-SGD)**: Provides full-data differential privacy, used for comparison.
- **PROPS (Progressively Private Self-Alignment)**: A multi-stage self-alignment strategy that refines noisy labels using intermediate models trained under privacy constraints.

---

## ğŸ”§ Features

- âœ… Privacy-preserving training using **label-only differential privacy** (via RR).
- âœ… Multi-stage alignment with **PROPS**, improving performance under tight privacy budgets.
- âœ… Tools for comparing **privacyâ€“utility trade-offs** across alignment strategies.
- âœ… Compatible with Hugging Face Transformers.

---

## ğŸ“Š Datasets Used

PROPS has been evaluated using the following **three preference datasets**:

1. **[AlpacaEval](https://huggingface.co/datasets/tatsu-lab/alpaca_eval)** â€“ Benchmark for evaluating LLMs with preference comparisons.
2. **[Anthropic HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)** â€“ Helpful and harmless human feedback dataset.
3. **[Truthy-DPO-v0.1](https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1)** â€“ Preference dataset tailored for Direct Preference Optimization (DPO).
   
> âš ï¸ **Note:** The same training script supports both **Truthy-DPO** and **HH-RLHF** out-of-the-box.  
> âš ï¸ **AlpacaEval** uses a slightly different format; support is included but **commented out** in the code.

---

## ğŸ§  Models Evaluated

We tested PROPS on the following pretrained language models:

- **[GPT2-Medium](https://huggingface.co/gpt2-medium)** (355M parameters)
- **[GPT2-Large](https://huggingface.co/gpt2-large)** (774M parameters)
- **[Pythia-1B](https://huggingface.co/EleutherAI/pythia-1b)** â€“ Open LLM from EleutherAI

---

## ğŸ—‚ï¸ Repository Structure

PROPS/

â”œâ”€â”€ DP-SGD/ # Code for DP-SGD alignment 

â”‚ â”œâ”€â”€ Trainer.py # Custom trainer with gradient clipping and noise

â”‚ â””â”€â”€ Preference_dataset.py # Dataset wrapper with DP_SGD mechanisms

â”œâ”€â”€ Eval/ # Evaluation utilities 

â”‚ â”œâ”€â”€ preference_dataset.py         âš ï¸(must be fully included)

â”‚ â”œâ”€â”€ utils.py                      âš ï¸(must be fully included)

â”‚ â”œâ”€â”€ gpt4-eval                     âš ï¸(must be fully included)

â”‚ â””â”€â”€ Win_Rate.ipynb 


â”œâ”€â”€ Label-DP/ # Randomized Response utilities

â”œâ”€â”€ PROPS/ # Core PROPS training logic

â”œâ”€â”€ SFT_Train/ # Scripts for supervised fine-tuning

â”‚ â””â”€â”€ configs/ # YAML config files for training/eval   âš ï¸(must be fully included, we provide a simple pythia1B example)

â”œâ”€â”€ README.md # This file
