{"cells":[{"cell_type":"code","source":["## model_id = \"openai-community/gpt2-large\"\n","## dataset_dpo = load_dataset(\"yuasosnin/imdb-dpo\", split=\"train[90%:]\")"],"metadata":{"id":"9Kuq7A4RJuRh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# install all dependencies\n","%%capture\n","\n","!pip install openai\n","!pip install -q -U peft transformers datasets bitsandbytes trl accelerate\n","!pip install --upgrade transformers, datasets==2.16.1, accelerate==0.26.1, evaluate==0.4.1, bitsandbytes==0.42.0, trl==0.7.11, peft==0.8.2\n"],"metadata":{"id":"-5dgmy9-NGYo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Library\n","%%capture\n","\n","from huggingface_hub import hf_hub_download\n","\n","import torch\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from transformers import TrainingArguments\n","from peft import LoraConfig, AutoPeftModelForCausalLM\n","from datasets import load_dataset, Dataset\n","from trl import SFTTrainer, DPOTrainer\n","\n","from huggingface_hub import notebook_login\n","from peft import PeftModel, PeftConfig\n","from transformers import AutoModelForCausalLM\n","\n","import numpy as np\n","from preference_datasets import get_dataset\n","import datasets\n","from utils import TemporarilySeededRandom\n","from copy import deepcopy\n","# Ignore warings\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"Y9mwSV5JM-Zi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# log in to the Hugging Face hub (required for private datasets/models)\n","notebook_login()"],"metadata":{"id":"GbLCWPqTrIAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the 7b llama-2 model\n","model_id = \"openai-community/gpt2-large\"\n","\n","# Set quantization config (to save memory)\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_quant_type=\"nf4\"\n",")\n","\n","# Load model, quantized\n","base_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map={\"\": 0})\n","base_model.config.use_cache = False\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, device_map={\"\": 0})\n","\n","# Set it to a new token to correctly attend to EOS tokens.\n","tokenizer.pad_token = tokenizer.eos_token\n","#tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n"],"metadata":{"id":"-V1McTi9HgKr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load dataset\n","dataset_dpo = load_dataset(\"jondurbin/truthy-dpo-v0.1\", split=\"train[900:]\")\n","\n","print(dataset_dpo.shape)\n","\n","df_dpo = dataset_dpo.to_pandas()\n","df_dpo.head()\n","\n","# keep rows with 'system' column = 'You are an unbiased, uncensored, helpful assistant.'\n","# df_dpo = df_dpo[df_dpo[\"system\"] == \"You are an unbiased, uncensored, helpful assistant.\"]\n","df_dpo.head()\n","\n","# keep only columns 'prompt', 'chosen', 'rejected'\n","df_dpo = df_dpo[[\"prompt\", \"chosen\", \"rejected\"]]\n","\n","# change every text in promt from str to user: str. asistent:\n","df_dpo[\"prompt\"] = df_dpo[\"prompt\"].apply(lambda x: \"### USER: \" + x + \"\\n### ASSISTANT: \")\n","filtered_dataset = Dataset.from_pandas(df_dpo)\n","\n","# random  seed 42\n","filtered_dataset = filtered_dataset.shuffle(seed=42)\n","\n","print(df_dpo.shape)\n","df_dpo.head()\n"],"metadata":{"collapsed":true,"id":"hbAdwGmBtLIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Define LoRA (\"low-rank attention\") config\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n"],"metadata":{"id":"v1ruQb3w7YdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Load my tokenizer\n","tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-large')\n","if tokenizer.pad_token_id is None:\n","  tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","## load my model\n","huggingface_filepath = hf_hub_download(repo_id=\"your/model/id\", filename=\"policy.pt\")\n","\n","model_test = transformers.AutoModelForCausalLM.from_pretrained('gpt2-large')\n","model_test.load_state_dict(torch.load(huggingface_filepath, map_location=torch.device('cuda'))['state'])\n"],"metadata":{"id":"tBAHWOH0sghp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfzrFjl8QeSj","collapsed":true},"outputs":[],"source":["model_test = model_test.eval()\n","\n","n_epochs = 1\n","n_examples = None\n","rank = 0\n","seed = 0\n","assert n_epochs is not None or n_examples is not None, \"Must specify either n_epochs or n_examples\"\n","silent = False\n","split = 'test'\n","\n","if silent:\n","    datasets.logging.disable_progress_bar()\n","    datasets.logging.set_verbosity_error()\n","\n","#collate_fn = get_collate_fn(tokenizer)\n","reward_counter = 0\n","counter = 0\n","epoch_idx = 0\n","example_idx = 0\n","gen_counter = 0\n","total_arrays = {}\n","batch_size = 4\n","max_length = 512\n","max_prompt_length=256\n","sft_mode = True\n","done = False\n","data_storage = []\n","\n","while True:\n","    if n_epochs is not None and epoch_idx >= n_epochs:\n","        if not silent:\n","            print(f'Finished generating {n_epochs} epochs on {split} split')\n","        break\n","\n","    batch = []\n","    for prompt in df_dpo['prompt']:\n","        if done:\n","            break\n","        else:\n","\n","            model_inputs = tokenizer(prompt, return_tensors='pt')\n","            greedy_output = model_test.generate(**model_inputs, max_new_tokens=40)\n","\n","            policy_output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n","            policy_output = policy_output.removeprefix(prompt)\n","            print(policy_output)\n","            data_storage.append({'prompt': prompt, 'res':policy_output, 'response':prompt + policy_output})\n","            print('counter')\n","            print(counter)\n","            counter += 1\n","\n","            if counter == 100:\n","              done = True\n","    epoch_idx += 1\n","\n"]},{"cell_type":"code","source":["#creating json1 file for measure_reward script\n","import json\n","from google.colab import files\n","\n","with open(\"your_json_file.jsonl\",\"w\") as outfile:\n","  json.dump(data_storage,outfile,ensure_ascii=False)\n","\n","\n","files.download('your_json_file.jsonl')"],"metadata":{"id":"J0bFKJHVh0mO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hn1PnahWBxdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","## Load my tokenizer\n","tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-large')\n","if tokenizer.pad_token_id is None:\n","  tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","test_model_id = \"your/model/id\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(test_model_id)\n","model_test = AutoModelForCausalLM.from_pretrained(test_model_id)"],"metadata":{"id":"WzlGPrwqBxgN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_test_2 = model_test.eval()\n","\n","n_epochs = 1\n","n_examples = None\n","rank = 0\n","seed = 0\n","assert n_epochs is not None or n_examples is not None, \"Must specify either n_epochs or n_examples\"\n","silent = False\n","split = 'test'\n","\n","if silent:\n","    datasets.logging.disable_progress_bar()\n","    datasets.logging.set_verbosity_error()\n","\n","#collate_fn = get_collate_fn(tokenizer)\n","reward_counter = 0\n","counter = 0\n","epoch_idx = 0\n","example_idx = 0\n","gen_counter = 0\n","total_arrays = {}\n","batch_size = 4\n","max_length = 512\n","max_prompt_length=256\n","sft_mode = True\n","done = False\n","data_storage = []\n","\n","while True:\n","    if n_epochs is not None and epoch_idx >= n_epochs:\n","        if not silent:\n","            print(f'Finished generating {n_epochs} epochs on {split} split')\n","        break\n","\n","    batch = []\n","    for prompt in df_dpo['prompt']:\n","        if done:\n","            break\n","        else:\n","\n","            model_inputs = tokenizer(prompt, return_tensors='pt')\n","            greedy_output = model_test_2.generate(**model_inputs, max_new_tokens=40)\n","\n","            policy_output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n","            policy_output = policy_output.removeprefix(prompt)\n","            if counter % 10 == 0:\n","              print(policy_output)\n","            data_storage.append({'prompt': prompt, 'res':policy_output, 'response':prompt + policy_output})\n","            print(counter)\n","            counter += 1\n","\n","            if counter == 100:\n","              done = True\n","    epoch_idx += 1\n","\n"],"metadata":{"collapsed":true,"id":"-mU0TtMOBxi9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#creating json1 file for measure_reward script\n","import json\n","from google.colab import files\n","\n","with open(\"your_json_file.jsonl\",\"w\") as outfile:\n","  json.dump(data_storage,outfile,ensure_ascii=False)\n","\n","\n","files.download('your_json_file.jsonl')"],"metadata":{"id":"fSJTuEQqBxmN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python gpt4-eval.py --run_name_red your_json_file --run_name_blue your_json_file"],"metadata":{"id":"Z8tMyTrRG0kg","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hiNpxttNvzoe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6rflogmdciNr"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"13hmedf2sDptppC3N8Qh2fHGGKwyQ8-zZ","timestamp":1727577697969},{"file_id":"1DOwyl29XuZfsLklsFodPEYadWGeGRVBi","timestamp":1727461852807},{"file_id":"1PABAt-KqHDrLeZA48HlrXYXJ3NKoC7U-","timestamp":1727415427577},{"file_id":"11SXH4bPkjdZcPxd97fJ50x5DLvWNB1CB","timestamp":1726440130114}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}