{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-5dgmy9-NGYo"},"outputs":[],"source":["# install all dependencies\n","%%capture\n","\n","!pip install openai\n","!pip install -q -U peft transformers datasets bitsandbytes trl accelerate\n","!pip install --upgrade transformers, datasets==2.16.1, accelerate==0.26.1, evaluate==0.4.1, bitsandbytes==0.42.0, trl==0.7.11, peft==0.8.2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9mwSV5JM-Zi"},"outputs":[],"source":["# Library\n","%%capture\n","\n","from huggingface_hub import hf_hub_download\n","\n","import torch\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from transformers import TrainingArguments\n","from peft import LoraConfig, AutoPeftModelForCausalLM\n","from datasets import load_dataset, Dataset\n","from trl import SFTTrainer, DPOTrainer\n","\n","from huggingface_hub import notebook_login\n","from peft import PeftModel, PeftConfig\n","from transformers import AutoModelForCausalLM\n","\n","import numpy as np\n","from preference_datasets import get_dataset\n","import datasets\n","from utils import TemporarilySeededRandom\n","from copy import deepcopy\n","# Ignore warings\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbLCWPqTrIAw"},"outputs":[],"source":["# log in to the Hugging Face hub (required for private datasets/models)\n","# notebook_login()\n","!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hbAdwGmBtLIy"},"outputs":[],"source":["# Load the dataset\n","dataset_dpo = load_dataset(\"jondurbin/truthy-dpo-v0.1\", split=\"train[900:]\")\n","\n","print(dataset_dpo.shape)\n","\n","df_dpo = dataset_dpo.to_pandas()\n","df_dpo.head()\n","\n","# keep rows with 'system' column = 'You are an unbiased, uncensored, helpful assistant.'\n","# df_dpo = df_dpo[df_dpo[\"system\"] == \"You are an unbiased, uncensored, helpful assistant.\"]\n","df_dpo.head()\n","\n","# keep only columns 'prompt', 'chosen', 'rejected'\n","df_dpo = df_dpo[[\"prompt\", \"chosen\", \"rejected\"]]\n","\n","# change every text in promt from str to user: str. asistent:\n","df_dpo[\"prompt\"] = df_dpo[\"prompt\"].apply(lambda x: \"### USER: \" + x + \"\\n### ASSISTANT: \")\n","filtered_dataset = Dataset.from_pandas(df_dpo)\n","\n","# random  seed 42\n","filtered_dataset = filtered_dataset.shuffle(seed=42)\n","\n","print(df_dpo.shape)\n","df_dpo.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1ruQb3w7YdV"},"outputs":[],"source":["\n","# Define LoRA (\"low-rank attention\") config\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBAHWOH0sghp"},"outputs":[],"source":["## Load my tokenizer\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","## Load my tokenizer\n","tokenizer = transformers.AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n","if tokenizer.pad_token_id is None:\n","  tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"00XqPjkv8rba"},"outputs":[],"source":["model_test = AutoModelForCausalLM.from_pretrained(\"your/huggingface/model\")\n","\n","# huggingface_filepath = hf_hub_download(repo_id=\"your/huggingface/model\", filename=\"policy.pt\")\n","# model_test = transformers.AutoModelForCausalLM.from_pretrained('gpt2-medium')\n","# model_test.load_state_dict(torch.load(huggingface_filepath, map_location=torch.device('cuda'))['state'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfzrFjl8QeSj"},"outputs":[],"source":["model_test = model_test.eval()\n","\n","n_epochs = 1\n","n_examples = None\n","rank = 0\n","seed = 0\n","assert n_epochs is not None or n_examples is not None, \"Must specify either n_epochs or n_examples\"\n","silent = False\n","split = 'test'\n","\n","if silent:\n","    datasets.logging.disable_progress_bar()\n","    datasets.logging.set_verbosity_error()\n","\n","#collate_fn = get_collate_fn(tokenizer)\n","reward_counter = 0\n","counter = 0\n","epoch_idx = 0\n","example_idx = 0\n","gen_counter = 0\n","total_arrays = {}\n","batch_size = 4\n","max_length = 512\n","max_prompt_length=256\n","sft_mode = True\n","done = False\n","data_storage = []\n","\n","while True:\n","    if n_epochs is not None and epoch_idx >= n_epochs:\n","        if not silent:\n","            print(f'Finished generating {n_epochs} epochs on {split} split')\n","        break\n","\n","    batch = []\n","    for prompt in df_dpo['prompt']:\n","        if done:\n","            break\n","        else:\n","\n","            model_inputs = tokenizer(prompt, return_tensors='pt')\n","            greedy_output = model_test.generate(**model_inputs, max_new_tokens=40)\n","\n","            policy_output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n","            policy_output = policy_output.removeprefix(prompt)\n","            if counter % 10 == 0:\n","              print(policy_output)\n","            data_storage.append({'prompt': prompt, 'res':policy_output, 'response':prompt + policy_output})\n","            print(counter)\n","            counter += 1\n","\n","            if counter == 100:\n","              done = True\n","    epoch_idx += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0bFKJHVh0mO"},"outputs":[],"source":["#creating json1 file for measure_reward script\n","import json\n","from google.colab import files\n","\n","with open(\"your_model_output.jsonl\",\"w\") as outfile:\n","  json.dump(data_storage,outfile,ensure_ascii=False)\n","\n","\n","files.download('your_model_output.jsonl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hn1PnahWBxdl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzlGPrwqBxgN"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","huggingface_filepath = hf_hub_download(repo_id=\"Another/model\", filename=\"policy.pt\")\n","model_test_2 = transformers.AutoModelForCausalLM.from_pretrained('gpt2-large')\n","model_test_2.load_state_dict(torch.load(huggingface_filepath, map_location=torch.device('cuda'))['state'])"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"-mU0TtMOBxi9"},"outputs":[],"source":["model_test_2 = model_test_2.eval()\n","\n","n_epochs = 1\n","n_examples = None\n","rank = 0\n","seed = 0\n","assert n_epochs is not None or n_examples is not None, \"Must specify either n_epochs or n_examples\"\n","silent = False\n","split = 'test'\n","\n","if silent:\n","    datasets.logging.disable_progress_bar()\n","    datasets.logging.set_verbosity_error()\n","\n","#collate_fn = get_collate_fn(tokenizer)\n","reward_counter = 0\n","counter = 0\n","epoch_idx = 0\n","example_idx = 0\n","gen_counter = 0\n","total_arrays = {}\n","batch_size = 4\n","max_length = 512\n","max_prompt_length=256\n","sft_mode = True\n","done = False\n","data_storage = []\n","\n","while True:\n","    if n_epochs is not None and epoch_idx >= n_epochs:\n","        if not silent:\n","            print(f'Finished generating {n_epochs} epochs on {split} split')\n","        break\n","\n","    batch = []\n","    for prompt in df_dpo['prompt']:\n","        if done:\n","            break\n","        else:\n","\n","            model_inputs = tokenizer(prompt, return_tensors='pt')\n","            greedy_output = model_test_2.generate(**model_inputs, max_new_tokens=40)\n","\n","            policy_output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n","            policy_output = policy_output.removeprefix(prompt)\n","            if counter % 10 == 0:\n","              print(policy_output)\n","            data_storage.append({'prompt': prompt, 'res':policy_output, 'response':prompt + policy_output})\n","            print(counter)\n","            counter += 1\n","\n","            if counter == 100:\n","              done = True\n","    epoch_idx += 1\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSJTuEQqBxmN"},"outputs":[],"source":["#creating json1 file for measure_reward script\n","import json\n","from google.colab import files\n","\n","with open(\"your_model_output_2.jsonl\",\"w\") as outfile:\n","  json.dump(data_storage,outfile,ensure_ascii=False)\n","\n","\n","files.download('your_model_output_2.jsonl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8QrdVpfuMcs"},"outputs":[],"source":["!python gpt4-eval.py --run_name_red your_model_output --run_name_blue your_model_output_2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gS9TZhJVuMfU"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"13hmedf2sDptppC3N8Qh2fHGGKwyQ8-zZ","timestamp":1727577697969},{"file_id":"1DOwyl29XuZfsLklsFodPEYadWGeGRVBi","timestamp":1727461852807},{"file_id":"1PABAt-KqHDrLeZA48HlrXYXJ3NKoC7U-","timestamp":1727415427577},{"file_id":"11SXH4bPkjdZcPxd97fJ50x5DLvWNB1CB","timestamp":1726440130114}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}